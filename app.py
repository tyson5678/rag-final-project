import streamlit as st
import os
import tempfile
import sys

# ================= é›²ç«¯è³‡æ–™åº«ä¿®æ­£ (ä¸€å®šè¦æ”¾åœ¨æœ€ä¸Šé¢) =================
# é€™æ˜¯ç‚ºäº†ä¿®å¾© Streamlit Cloud ä¸Š ChromaDB æœƒé‡åˆ°çš„ SQLite ç‰ˆæœ¬å•é¡Œ
# å¦‚æœæ²’æœ‰é€™æ®µï¼Œä¸Šç·šå¾Œæœƒå ± "sqlite3 version too old" çš„éŒ¯èª¤
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass
# ===============================================================

# åŒ¯å…¥ LangChain ç›¸é—œå¥—ä»¶
from langchain_groq import ChatGroq
from langchain_community.document_loaders import PyPDFLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_chroma import Chroma
from langchain.chains import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# æ¶ˆé™¤ Tokenizers çš„å¹³è¡Œé‹ç®—è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ================= è¨­å®šå€ï¼šAPI Key ç®¡ç† =================
# å„ªå…ˆå˜—è©¦å¾ Streamlit Secrets è®€å– (é›²ç«¯æ¨¡å¼)
# å¦‚æœè®€ä¸åˆ° (ä¾‹å¦‚åœ¨æœ¬æ©Ÿè·‘)ï¼Œå‰‡ä½¿ç”¨ä¸‹æ–¹çš„é è¨­ Key (ä½†å»ºè­°ä¸Šå‚³ GitHub å‰æŠŠä¸‹æ–¹çœŸå¯¦ Key åˆªæ‰)
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except:
    # âš ï¸ æ³¨æ„ï¼šä¸Šå‚³ GitHub æ™‚ï¼Œå»ºè­°å°‡å¼•è™Ÿå…§çš„çœŸå¯¦ Key åˆªé™¤ï¼Œæ”¹ç‚ºæç¤ºæ–‡å­—
    GROQ_API_KEY = "è«‹å¡«å…¥Key"
# ====================================================

# 1. è¨­å®šç¶²é æ¨™é¡Œã€åœ–ç¤ºèˆ‡ç‰ˆé¢
st.set_page_config(
    page_title="AI çŸ¥è­˜åº«åŠ©æ‰‹ (Llama 3.3)", 
    page_icon="ğŸ“š",
    layout="wide"
)

st.title("ğŸ“š å°ˆå±¬ PDF çŸ¥è­˜å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ Powered by Groq Llama 3 & LangChain | æ”¯æ´åƒæ•¸èª¿æ ¡ (Fine-tuning)")

# 2. åˆå§‹åŒ– session state
if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# ================= å´é‚Šæ¬„ï¼šåŠŸèƒ½èˆ‡åƒæ•¸å€ =================
with st.sidebar:
    st.header("ğŸ“ 1. è³‡æ–™ä¸Šå‚³")
    
    # æ”¯æ´å¤šæª”æ¡ˆä¸Šå‚³
    uploaded_files = st.file_uploader(
        "è«‹ä¸Šå‚³ PDF æ–‡ä»¶", 
        type="pdf", 
        accept_multiple_files=True
    )
    
    # è™•ç†ä¸Šå‚³é‚è¼¯
    if uploaded_files and st.session_state.vector_db is None:
        with st.spinner("â˜ï¸ æ­£åœ¨é›²ç«¯åˆ†æ PDF..."):
            try:
                all_splits = []
                for uploaded_file in uploaded_files:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    loader = PyPDFLoader(tmp_path)
                    docs = loader.load()
                    
                    for doc in docs:
                        doc.metadata["source_filename"] = uploaded_file.name
                    
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=1000, 
                        chunk_overlap=100
                    )
                    splits = text_splitter.split_documents(docs)
                    all_splits.extend(splits)
                    
                    os.remove(tmp_path)

                if all_splits:
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                    vector_db = Chroma.from_documents(documents=all_splits, embedding=embeddings)
                    st.session_state.vector_db = vector_db
                    st.success(f"âœ… æˆåŠŸè™•ç† {len(uploaded_files)} ä»½æ–‡ä»¶ï¼")
                else:
                    st.warning("âš ï¸ è®€å–åˆ°çš„æ–‡ä»¶å…§å®¹ç‚ºç©ºã€‚")

            except Exception as e:
                st.error(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")

    # ğŸŒŸ æ–°å¢åŠŸèƒ½ï¼šæ¨¡å‹åƒæ•¸èª¿æ•´å€
    st.divider()
    st.header("âš™ï¸ 2. é€²éšåƒæ•¸è¨­å®š")
    
    # åƒæ•¸ 1: Temperature
    temperature = st.slider(
        "æ¨¡å‹å‰µæ„åº¦ (Temperature)", 
        min_value=0.0, 
        max_value=1.0, 
        value=0.1, 
        step=0.1,
        help="æ•¸å€¼è¶Šä½ (0.1)ï¼Œå›ç­”è¶Šåš´è¬¹ã€ç›´æ¥å¼•ç”¨åŸæ–‡ï¼›æ•¸å€¼è¶Šé«˜ (0.9)ï¼Œå›ç­”è¶Šæœ‰å‰µæ„ä½†å¯èƒ½ç”¢ç”Ÿå¹»è¦ºã€‚"
    )
    
    # åƒæ•¸ 2: Top-K
    k_value = st.slider(
        "æª¢ç´¢æ®µè½æ•¸ (Top-K)", 
        min_value=2, 
        max_value=20, 
        value=4, 
        step=1,
        help="æ±ºå®š AI ä¸€æ¬¡åƒè€ƒå¤šå°‘å€‹æœ€ç›¸é—œçš„æ®µè½ã€‚è¨­ç‚º 4 ä»£è¡¨ AI æœƒé–±è®€ 4 å€‹æœ€ç›¸é—œçš„ç‰‡æ®µä¾†å›ç­”ä½ ã€‚"
    )

    # é‡ç½®æŒ‰éˆ•
    st.divider()
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±ç´€éŒ„"):
        st.session_state.messages = []
        st.rerun()

    if st.button("ğŸ”„ é‡ç½®æ‰€æœ‰æ–‡ä»¶"):
        st.session_state.messages = []
        st.session_state.vector_db = None
        st.rerun()

# ================= ä¸»ç•«é¢ï¼šèŠå¤©å€ =================

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥é—œæ–¼é€™ä»½æ–‡ä»¶çš„å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if st.session_state.vector_db is not None:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("ğŸ¤– AI æ­£åœ¨é›²ç«¯æ€è€ƒä¸­...")
            
            try:
                # ğŸŒŸ ä½¿ç”¨ secrets æˆ– fallback key
                llm = ChatGroq(
                    groq_api_key=GROQ_API_KEY, 
                    model_name="llama-3.3-70b-versatile",
                    temperature=temperature 
                )
                
                qa_prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„å­¸è¡“åŠ©ç†ã€‚è«‹æ ¹æ“šä¸‹æ–¹çš„ã€ä¸Šä¸‹æ–‡ã€‘å…§å®¹ä¾†å›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
                å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè«‹èª å¯¦èªªä¸çŸ¥é“ï¼Œä¸è¦ç·¨é€ ç­”æ¡ˆã€‚
                
                ã€ä¸Šä¸‹æ–‡ã€‘:
                {context}
                
                ã€å•é¡Œã€‘:
                {input}
                
                è«‹å‹™å¿…ä½¿ç”¨ã€Œå°ç£ç¹é«”ä¸­æ–‡ã€å›ç­”ï¼Œä¸¦ä¿æŒèªæ°£å°ˆæ¥­ã€æ¢ç†åˆ†æ˜ï¼š
                """)

                # ğŸŒŸ ä½¿ç”¨è€…èª¿æ•´çš„ k_value æœƒåœ¨é€™è£¡ç”Ÿæ•ˆ
                retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": k_value})
                
                document_chain = create_stuff_documents_chain(llm, qa_prompt)
                retrieval_chain = create_retrieval_chain(retriever, document_chain)
                
                response = retrieval_chain.invoke({"input": prompt})
                answer = response['answer']
                context_docs = response['context']
                
                message_placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                with st.expander(f"ğŸ” æŸ¥çœ‹åƒè€ƒä¾†æº (å…±åƒè€ƒ {len(context_docs)} å€‹ç‰‡æ®µ)"):
                    for i, doc in enumerate(context_docs):
                        source_name = doc.metadata.get("source_filename", "æœªçŸ¥æ–‡ä»¶")
                        page_num = doc.metadata.get("page", 0) + 1
                        
                        st.markdown(f"**ğŸ“„ ä¾†æº {i+1}: {source_name} (ç¬¬ {page_num} é )**")
                        st.text(doc.page_content)
                        st.divider()
                
            except Exception as e:
                message_placeholder.markdown(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
                st.error("è«‹æª¢æŸ¥ API Key æ˜¯å¦æ­£ç¢ºã€‚")
    else:
        with st.chat_message("assistant"):
            st.warning("âš ï¸ è«‹å…ˆåœ¨å·¦å´ä¸Šå‚³ PDF æª”æ¡ˆï¼")