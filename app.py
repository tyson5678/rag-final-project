import streamlit as st
import os
import tempfile
import sys
import logging

# ================= é›²ç«¯è³‡æ–™åº«ä¿®æ­£ =================
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass
# ===============================================

# åŒ¯å…¥ LangChain ç›¸é—œå¥—ä»¶
try:
    from langchain_groq import ChatGroq
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
    from langchain.chains import create_retrieval_chain
    from langchain.chains.combine_documents import create_stuff_documents_chain
    from langchain_core.prompts import ChatPromptTemplate
    
    # ğŸŒŸ æ–°å¢ï¼šå¤šé‡æŸ¥è©¢æª¢ç´¢å™¨ (è®“ AI å¹«ä½ å¤šå•å¹¾æ¬¡)
    from langchain.retrievers.multi_query import MultiQueryRetriever
    
except ImportError as e:
    st.error(f"âŒ ç³»çµ±å•Ÿå‹•å¤±æ•—ï¼è©³ç´°éŒ¯èª¤: {e}")
    st.stop()

# æ¶ˆé™¤è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"
# è¨­å®š Log é¿å… MultiQuery è¼¸å‡ºå¤ªå¤šé›œè¨Š
logging.getLogger("langchain.retrievers.multi_query").setLevel(logging.INFO)

# ================= API Key è¨­å®š =================
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except:
    GROQ_API_KEY = "è«‹å¡«å…¥ä½ çš„API_KEY"

# ================= é é¢è¨­å®š =================
st.set_page_config(page_title="AI ç²¾æº–çŸ¥è­˜åº«", page_icon="ğŸ¯", layout="wide")
st.title("ğŸ¯ AI ç²¾æº– PDF å•ç­”åŠ©æ‰‹")
st.caption("ğŸš€ å‡ç´šç‰ˆï¼šæ”¯æ´ Multi-Query å¤šé‡æª¢ç´¢èˆ‡ç²¾ç´°åˆ‡åˆ†")

# åˆå§‹åŒ– Session
if "messages" not in st.session_state:
    st.session_state.messages = []
if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

# ================= å´é‚Šæ¬„ =================
with st.sidebar:
    st.header("ğŸ“ è³‡æ–™è™•ç†è¨­å®š")
    
    uploaded_files = st.file_uploader("ä¸Šå‚³ PDF", type="pdf", accept_multiple_files=True)
    
    # ğŸŒŸ å„ªåŒ–é» 1ï¼šè®“ä½¿ç”¨è€…æ±ºå®šåˆ‡åˆ†å¤§å° (è¶Šå°è¶Šç²¾æº–)
    chunk_size = st.slider("åˆ‡åˆ†å¤§å° (Chunk Size)", 200, 1000, 400, 50, help="æ•¸å€¼è¶Šå°ï¼Œåˆ‡åˆ†è¶Šç´°ï¼Œå°ç´°ç¯€å•ç­”è¶Šç²¾æº–ï¼›æ•¸å€¼è¶Šå¤§ï¼Œå°æ‘˜è¦å‹å•ç­”è¶Šå¥½ã€‚")
    
    if uploaded_files and st.session_state.vector_db is None:
        with st.spinner("â˜ï¸ æ­£åœ¨é€²è¡Œç²¾ç´°åŒ–åˆ†æ..."):
            try:
                all_splits = []
                for uploaded_file in uploaded_files:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    loader = PyPDFLoader(tmp_path)
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata["source_filename"] = uploaded_file.name
                    
                    # ğŸŒŸ ä½¿ç”¨æ›´ç´°çš„åˆ‡åˆ†è¨­å®š
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size,  # ä½¿ç”¨æ»‘æ¡¿çš„å€¼
                        chunk_overlap=100,      # é‡ç–Šéƒ¨åˆ†ä¿æŒä¸Šä¸‹æ–‡
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""] # é‡å°ä¸­æ–‡å„ªåŒ–åˆ‡å‰²ç¬¦
                    )
                    splits = text_splitter.split_documents(docs)
                    all_splits.extend(splits)
                    os.remove(tmp_path)

                if all_splits:
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                    vector_db = Chroma.from_documents(documents=all_splits, embedding=embeddings)
                    st.session_state.vector_db = vector_db
                    st.success(f"âœ… ç²¾ç´°è™•ç†å®Œæˆï¼å…±åˆ‡åˆ†æˆ {len(all_splits)} å€‹ç‰‡æ®µ")
            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {e}")

    st.divider()
    st.header("âš™ï¸ æª¢ç´¢å¢å¼·è¨­å®š")
    
    # ğŸŒŸ å„ªåŒ–é» 2ï¼šé–‹å•Ÿ Multi-Query é–‹é—œ
    use_multiquery = st.toggle("å•Ÿç”¨å¤šé‡æŸ¥è©¢ (Multi-Query)", value=True, help="AI æœƒè‡ªå‹•ç”¢ç”Ÿ 3 å€‹ä¸åŒç‰ˆæœ¬çš„å•æ³•å»æœå°‹ï¼Œèƒ½å¤§å¹…æå‡æº–ç¢ºåº¦ï¼Œä½†é€Ÿåº¦æœƒç¨æ…¢ã€‚")
    
    temperature = st.slider("å‰µæ„åº¦", 0.0, 1.0, 0.1)
    k_value = st.slider("åƒè€ƒæ®µè½æ•¸", 2, 10, 5) # é è¨­æé«˜åˆ° 5

    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()
    if st.button("ğŸ”„ é‡ç½®æ–‡ä»¶"):
        st.session_state.messages = []
        st.session_state.vector_db = None
        st.rerun()

# ================= ä¸»ç•«é¢ =================

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if st.session_state.vector_db:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("ğŸ” AI æ­£åœ¨å¤šè§’åº¦æª¢ç´¢è³‡æ–™ä¸­...")
            
            try:
                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile", temperature=temperature)
                
                # 1. è¨­å®šåŸºç¤æª¢ç´¢å™¨
                base_retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": k_value})
                
                # ğŸŒŸ å„ªåŒ–é» 2 å¯¦ä½œï¼šæ ¹æ“šé–‹é—œæ±ºå®šæ˜¯å¦ä½¿ç”¨ Multi-Query
                if use_multiquery:
                    # é€™æ˜¯ä¸€å€‹æœƒè‡ªå‹•å¹«ä½ æ›å¥è©±èªªçš„æª¢ç´¢å™¨
                    retriever = MultiQueryRetriever.from_llm(
                        retriever=base_retriever,
                        llm=llm
                    )
                else:
                    retriever = base_retriever

                # ğŸŒŸ å„ªåŒ–é» 3ï¼šæ›´åš´æ ¼çš„ Prompt (è¦æ±‚å¼•ç”¨è­‰æ“š)
                qa_prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„å­¸è¡“åŠ©ç†ã€‚è«‹åš´æ ¼æ ¹æ“šã€ä¸Šä¸‹æ–‡ã€‘å›ç­”å•é¡Œã€‚
                
                ã€ä»»å‹™è¦æ±‚ã€‘ï¼š
                1. ç­”æ¡ˆå¿…é ˆä¾†è‡ªä¸‹æ–¹æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œä¸è¦åŠ å…¥è‡ªå·±çš„å¤–éƒ¨çŸ¥è­˜ã€‚
                2. å¦‚æœä¸Šä¸‹æ–‡ä¸­åŒ…å«å…·é«”æ•¸æ“šã€æ—¥æœŸæˆ–äººåï¼Œè«‹ç²¾ç¢ºåˆ—å‡ºã€‚
                3. å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè«‹ç›´æ¥å›ç­”ã€Œæ–‡ä»¶ä¸­æœªæåŠæ­¤è³‡è¨Šã€ã€‚
                4. è«‹æ¢ç†åˆ†æ˜åœ°åˆ—é»å›ç­”ã€‚

                ã€ä¸Šä¸‹æ–‡ã€‘:
                {context}
                
                ã€å•é¡Œã€‘:
                {input}
                
                è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
                """)

                document_chain = create_stuff_documents_chain(llm, qa_prompt)
                retrieval_chain = create_retrieval_chain(retriever, document_chain)
                
                # åŸ·è¡Œ
                response = retrieval_chain.invoke({"input": prompt})
                answer = response['answer']
                
                message_placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # é¡¯ç¤ºä¾†æº
                with st.expander("ğŸ” æª¢è¦–ç²¾æº–åƒè€ƒä¾†æº"):
                    for i, doc in enumerate(response['context']):
                        st.markdown(f"**ç‰‡æ®µ {i+1} ({doc.metadata.get('source_filename')} p.{doc.metadata.get('page',0)+1})**")
                        st.info(doc.page_content) # ä½¿ç”¨ info æ¡†è®“æ–‡å­—æ›´æ˜é¡¯
            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {e}")
                if "429" in str(e):
                    st.warning("âš ï¸ è«‹æ±‚éæ–¼é »ç¹ (Rate Limit)ï¼Œè«‹ç¨ç­‰å¹¾ç§’å†è©¦ï¼Œæˆ–æ˜¯é—œé–‰ Multi-Query åŠŸèƒ½ã€‚")
    else:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³ PDF")