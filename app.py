import streamlit as st
import os
import sys

# ================= 1. é›²ç«¯è³‡æ–™åº«ä¿®æ­£ =================
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# ================= 2. è¨­å®šé é¢ =================
st.set_page_config(page_title="AI çŸ¥è­˜åº«åŠ©æ‰‹", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š å°ˆå±¬æ–‡ä»¶å•ç­”åŠ©æ‰‹ (PDF + Word)")

# ================= 3. å®‰å…¨è¼‰å…¥å¥—ä»¶ =================
try:
    import langchain
    from langchain_groq import ChatGroq
    from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader # ğŸŒŸ æ–°å¢ Word è®€å–å™¨
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
    
    # å˜—è©¦åŒ¯å…¥ Chain
    try:
        from langchain.chains import create_retrieval_chain
        from langchain.chains.combine_documents import create_stuff_documents_chain
    except ImportError:
        from langchain.chains.retrieval import create_retrieval_chain
        
    from langchain.retrievers.multi_query import MultiQueryRetriever
    from langchain_core.prompts import ChatPromptTemplate
    
except ImportError as e:
    st.error(f"âŒ ç³»çµ±å•Ÿå‹•å¤±æ•—ï¼éŒ¯èª¤åŸå› : {e}")
    st.stop()

# æ¶ˆé™¤è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ================= 4. API Key è¨­å®š =================
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except:
    GROQ_API_KEY = "è«‹å¡«å…¥Key"

# ================= 5. ä¸»ç¨‹å¼é‚è¼¯ =================

if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

with st.sidebar:
    st.header("ğŸ“ è³‡æ–™ä¸Šå‚³")
    
    # ğŸŒŸ ä¿®æ”¹é» 1ï¼šå…è¨± pdf å’Œ docx å…©ç¨®é¡å‹
    uploaded_files = st.file_uploader(
        "è«‹ä¸Šå‚³æ–‡ä»¶ (æ”¯æ´ PDF èˆ‡ Word)", 
        type=["pdf", "docx"], 
        accept_multiple_files=True
    )
    
    # ğŸŒŸ ä¿®æ”¹é» 2ï¼šåŠ å…¥åˆ‡åˆ†å¤§å°æ»‘æ¡¿
    chunk_size = st.slider("åˆ‡åˆ†å¤§å° (Chunk Size)", 200, 1000, 400, 50)
    
    if uploaded_files and st.session_state.vector_db is None:
        with st.spinner("â˜ï¸ æ­£åœ¨åˆ†ææ–‡ä»¶ (PDF/Word)..."):
            try:
                import tempfile
                all_splits = []
                for uploaded_file in uploaded_files:
                    # åˆ¤æ–·å‰¯æª”å
                    file_name = uploaded_file.name
                    file_extension = os.path.splitext(file_name)[1].lower()
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix=file_extension) as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    # ğŸŒŸ ä¿®æ”¹é» 3ï¼šæ™ºæ…§åˆ¤æ–·ä½¿ç”¨å“ªç¨®è®€å–å™¨
                    if file_extension == ".pdf":
                        loader = PyPDFLoader(tmp_path)
                    elif file_extension == ".docx":
                        loader = Docx2txtLoader(tmp_path)
                    else:
                        continue # è·³éä¸æ”¯æ´çš„æ ¼å¼
                        
                    docs = loader.load()
                    
                    for doc in docs:
                        doc.metadata["source_filename"] = file_name
                    
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=chunk_size, 
                        chunk_overlap=100,
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
                    )
                    splits = text_splitter.split_documents(docs)
                    all_splits.extend(splits)
                    os.remove(tmp_path)

                if all_splits:
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                    vector_db = Chroma.from_documents(documents=all_splits, embedding=embeddings)
                    st.session_state.vector_db = vector_db
                    st.success(f"âœ… æˆåŠŸè™•ç† {len(uploaded_files)} ä»½æ–‡ä»¶ï¼")
                else:
                    st.warning("âš ï¸ æª”æ¡ˆå…§å®¹ç‚ºç©º")
            except Exception as e:
                st.error(f"âŒ è³‡æ–™è™•ç†éŒ¯èª¤: {e}")

    st.divider()
    st.header("âš™ï¸ é€²éšè¨­å®š")
    use_multiquery = st.toggle("å•Ÿç”¨å¤šé‡æŸ¥è©¢ (Multi-Query)", value=True)
    temperature = st.slider("å‰µæ„åº¦ (Temperature)", 0.0, 1.0, 0.1, 0.1)
    k_value = st.slider("æª¢ç´¢æ•¸ (Top-K)", 2, 10, 5)

    st.divider()
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()
    if st.button("ğŸ”„ é‡ç½®æ–‡ä»¶"):
        st.session_state.messages = []
        st.session_state.vector_db = None
        st.rerun()

# ================= èŠå¤©å€ =================

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if st.session_state.vector_db:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            message_placeholder.markdown("ğŸ” AI æ­£åœ¨æª¢ç´¢ä¸­...")
            try:
                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile", temperature=temperature)
                
                base_retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": k_value})
                
                if use_multiquery:
                    retriever = MultiQueryRetriever.from_llm(retriever=base_retriever, llm=llm)
                else:
                    retriever = base_retriever
                
                qa_prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€å€‹ç²¾æº–çš„å­¸è¡“åŠ©ç†ã€‚è«‹åš´æ ¼æ ¹æ“šã€ä¸Šä¸‹æ–‡ã€‘å›ç­”å•é¡Œã€‚
                ã€ä»»å‹™è¦æ±‚ã€‘ï¼š
                1. ç­”æ¡ˆå¿…é ˆä¾†è‡ªä¸‹æ–¹æä¾›çš„ä¸Šä¸‹æ–‡ï¼Œä¸è¦åŠ å…¥è‡ªå·±çš„å¤–éƒ¨çŸ¥è­˜ã€‚
                2. å¦‚æœä¸Šä¸‹æ–‡ä¸­åŒ…å«å…·é«”æ•¸æ“šã€æ—¥æœŸæˆ–äººåï¼Œè«‹ç²¾ç¢ºåˆ—å‡ºã€‚
                3. å¦‚æœç­”æ¡ˆä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œè«‹ç›´æ¥å›ç­”ã€Œæ–‡ä»¶ä¸­æœªæåŠæ­¤è³‡è¨Šã€ã€‚
                
                ã€ä¸Šä¸‹æ–‡ã€‘:{context}
                ã€å•é¡Œã€‘:{input}
                è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
                """)

                document_chain = create_stuff_documents_chain(llm, qa_prompt)
                retrieval_chain = create_retrieval_chain(retriever, document_chain)
                
                response = retrieval_chain.invoke({"input": prompt})
                answer = response['answer']
                
                message_placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                with st.expander("ğŸ” æª¢è¦–åƒè€ƒä¾†æº"):
                    for i, doc in enumerate(response['context']):
                        st.markdown(f"**ä¾†æº {i+1}: {doc.metadata.get('source_filename')}**")
                        st.info(doc.page_content)
            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {e}")
                if "429" in str(e):
                    st.warning("âš ï¸ è«‹æ±‚éæ–¼é »ç¹ï¼Œè«‹ç¨ç­‰ã€‚")
    else:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³ PDF æˆ– Word æª”æ¡ˆ")