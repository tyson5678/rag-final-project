import streamlit as st
import os
import sys
import tempfile

# ================= 1. é›²ç«¯è³‡æ–™åº«ä¿®æ­£ (ä¿æŒ) =================
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# ================= 2. é é¢è³ªæ„Ÿè¨­å®š =================
st.set_page_config(
    page_title="AI çŸ¥è­˜åº«åŠ©æ‰‹", 
    page_icon="ğŸ“‘", 
    layout="wide",
    initial_sidebar_state="expanded"
)

# æ¨™é¡Œè¨­è¨ˆï¼šç°¡ç´„æœ‰åŠ›
st.title("ğŸ“‘ å°ˆå±¬æ–‡ä»¶å•ç­”åŠ©æ‰‹")
st.markdown("##### æ”¯æ´ PDF èˆ‡ Word Â· æ™ºæ…§æª¢ç´¢ Â· ç²¾æº–å›ç­”")

# ================= 3. å®‰å…¨è¼‰å…¥å¥—ä»¶ =================
try:
    import langchain
    from langchain_groq import ChatGroq
    from langchain_community.document_loaders import PyPDFLoader, Docx2txtLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
    # å˜—è©¦åŒ¯å…¥ Chain
    try:
        from langchain.chains import create_retrieval_chain
        from langchain.chains.combine_documents import create_stuff_documents_chain
    except ImportError:
        from langchain.chains.retrieval import create_retrieval_chain
    from langchain_core.prompts import ChatPromptTemplate
    
except ImportError as e:
    st.error(f"âŒ ç³»çµ±å•Ÿå‹•å¤±æ•—ï¼åŸå› : {e}")
    st.stop()

# æ¶ˆé™¤è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ================= 4. API Key =================
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except:
    GROQ_API_KEY = "è«‹å¡«å…¥Key"

# ================= 5. æ ¸å¿ƒé‚è¼¯ =================

if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

with st.sidebar:
    st.header("ğŸ—‚ï¸ è³‡æ–™ä¸Šå‚³")
    
    # ç°¡ç´„çš„ä¸Šå‚³å€ï¼Œä½†æ”¯æ´å…©ç¨®æ ¼å¼
    uploaded_files = st.file_uploader(
        "ä¸Šå‚³æ–‡ä»¶ (PDF / Word)", 
        type=["pdf", "docx"], 
        accept_multiple_files=True
    )
    
    if uploaded_files and st.session_state.vector_db is None:
        with st.spinner("âœ¨ AI æ­£åœ¨åˆ†ææ–‡ä»¶ä¸­..."):
            try:
                all_splits = []
                for uploaded_file in uploaded_files:
                    file_name = uploaded_file.name
                    file_ext = os.path.splitext(file_name)[1].lower()
                    
                    with tempfile.NamedTemporaryFile(delete=False, suffix=file_ext) as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    # æ™ºæ…§åˆ¤æ–·è®€å–å™¨
                    if file_ext == ".pdf":
                        loader = PyPDFLoader(tmp_path)
                    elif file_ext == ".docx":
                        loader = Docx2txtLoader(tmp_path)
                    else:
                        continue
                        
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata["source_filename"] = file_name
                    
                    # ä½¿ç”¨å›ºå®šçš„æœ€ä½³åƒæ•¸ (Chunk=500)ï¼Œè®“ä»‹é¢æ›´ä¹¾æ·¨
                    text_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=500, 
                        chunk_overlap=50,
                        separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", " ", ""]
                    )
                    splits = text_splitter.split_documents(docs)
                    all_splits.extend(splits)
                    os.remove(tmp_path)

                if all_splits:
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                    vector_db = Chroma.from_documents(documents=all_splits, embedding=embeddings)
                    st.session_state.vector_db = vector_db
                    st.toast(f"âœ… å·²è™•ç† {len(uploaded_files)} ä»½æ–‡ä»¶", icon="ğŸ‰")
                else:
                    st.warning("âš ï¸ æª”æ¡ˆå…§å®¹ç‚ºç©º")
            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {e}")

    st.divider()
    st.header("âš™ï¸ åƒæ•¸è¨­å®š")
    
    # åªä¿ç•™é€™å…©å€‹æœ€é‡è¦çš„æ»‘æ¡¿
    temperature = st.slider("æ¨¡å‹å‰µæ„åº¦", 0.0, 1.0, 0.1, 0.1)
    k_value = st.slider("åƒè€ƒæ®µè½æ•¸", 2, 8, 4)

    st.markdown("") # åŠ ä¸€é»ç•™ç™½
    col1, col2 = st.columns(2)
    with col1:
        if st.button("ğŸ—‘ï¸ æ¸…ç©ºå°è©±", use_container_width=True):
            st.session_state.messages = []
            st.rerun()
    with col2:
        if st.button("ğŸ”„ é‡ç½®æ–‡ä»¶", use_container_width=True):
            st.session_state.messages = []
            st.session_state.vector_db = None
            st.rerun()

# ================= èŠå¤©ä»‹é¢ =================

# é¡¯ç¤ºæ­¡è¿è¨Šæ¯ (å¦‚æœæ²’è¨Šæ¯æ™‚)
if not st.session_state.messages:
    st.info("ğŸ‘‹ å—¨ï¼è«‹åœ¨å·¦å´ä¸Šå‚³æ–‡ä»¶ï¼Œç„¶å¾Œå•æˆ‘ä»»ä½•å•é¡Œã€‚")

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if st.session_state.vector_db:
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            # message_placeholder.markdown("Thinking...") # è®“ç•«é¢æ›´ä¹¾æ·¨ï¼Œä¸é¡¯ç¤º Thinking æ–‡å­—
            
            try:
                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile", temperature=temperature)
                
                # æç¤ºè©å„ªåŒ–ï¼šæ›´ç°¡æ½”å°ˆæ¥­
                qa_prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©ç†ã€‚è«‹æ ¹æ“šä»¥ä¸‹ã€ä¸Šä¸‹æ–‡ã€‘å›ç­”å•é¡Œã€‚
                1. ç­”æ¡ˆå¿…é ˆæœ‰æ†‘æœ‰æ“šã€‚
                2. è‹¥ç„¡ç›¸é—œè³‡è¨Šï¼Œè«‹èª å¯¦å›ç­”ã€Œæ–‡ä»¶ä¸­æœªæåŠã€ã€‚
                3. è«‹ç”¨å°ç£ç¹é«”ä¸­æ–‡å›ç­”ã€‚
                
                ã€ä¸Šä¸‹æ–‡ã€‘:
                {context}
                
                ã€å•é¡Œã€‘:
                {input}
                """)

                retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": k_value})
                document_chain = create_stuff_documents_chain(llm, qa_prompt)
                retrieval_chain = create_retrieval_chain(retriever, document_chain)
                
                response = retrieval_chain.invoke({"input": prompt})
                answer = response['answer']
                
                message_placeholder.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                # å¼•ç”¨ä¾†æºæ”¹æˆç°¡æ½”çš„ç°è‰²å°å­—
                with st.expander("åƒè€ƒä¾†æº (Source)"):
                    for i, doc in enumerate(response['context']):
                        st.caption(f"ğŸ“„ **{doc.metadata.get('source_filename')}** (Page {doc.metadata.get('page',0)+1})")
                        st.text(doc.page_content[:100] + "...")
                        st.divider()

            except Exception as e:
                st.error(f"âŒ éŒ¯èª¤: {e}")
                if "401" in str(e):
                    st.warning("API Key ç•°å¸¸ï¼Œè«‹æª¢æŸ¥ Secretsã€‚")
    else:
        st.toast("è«‹å…ˆä¸Šå‚³æ–‡ä»¶å–”ï¼", icon="âš ï¸")