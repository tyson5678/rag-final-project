import streamlit as st
import os
import sys

# ================= 1. é›²ç«¯è³‡æ–™åº«ä¿®æ­£ =================
try:
    __import__('pysqlite3')
    import sys
    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')
except ImportError:
    pass

# ================= 2. è¨­å®šé é¢ (æ”¾åœ¨æœ€å‰é¢ä»¥å…å ±éŒ¯) =================
st.set_page_config(page_title="AI çŸ¥è­˜åº«åŠ©æ‰‹", page_icon="ğŸ“š", layout="wide")
st.title("ğŸ“š å°ˆå±¬ PDF çŸ¥è­˜å•ç­”åŠ©æ‰‹")

# ================= 3. å®‰å…¨è¼‰å…¥å¥—ä»¶ (åµéŒ¯æ¨¡å¼) =================
try:
    import langchain
    from langchain_groq import ChatGroq
    from langchain_community.document_loaders import PyPDFLoader
    from langchain_text_splitters import RecursiveCharacterTextSplitter
    from langchain_huggingface import HuggingFaceEmbeddings
    from langchain_chroma import Chroma
    
    # å˜—è©¦åŒ¯å…¥ Chainï¼Œå¦‚æœå¤±æ•—æœƒé¡¯ç¤ºç‰ˆæœ¬è™Ÿ
    try:
        from langchain.chains import create_retrieval_chain
        from langchain.chains.combine_documents import create_stuff_documents_chain
    except ImportError:
        # å¦‚æœæ–°è·¯å¾‘å¤±æ•—ï¼Œå˜—è©¦èˆŠè·¯å¾‘ (Fallback)
        from langchain.chains.retrieval import create_retrieval_chain
        
    from langchain_core.prompts import ChatPromptTemplate
    
except ImportError as e:
    st.error(f"âŒ ç³»çµ±å•Ÿå‹•å¤±æ•—ï¼")
    st.error(f"éŒ¯èª¤åŸå› : {e}")
    st.warning(f"ç›®å‰å®‰è£çš„ LangChain ç‰ˆæœ¬: {langchain.__version__}")
    st.stop()

# æ¶ˆé™¤ Tokenizers çš„å¹³è¡Œé‹ç®—è­¦å‘Š
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# ================= 4. API Key è¨­å®š =================
try:
    GROQ_API_KEY = st.secrets["GROQ_API_KEY"]
except:
    GROQ_API_KEY = "è«‹å¡«å…¥Key"

# ================= 5. ä¸»ç¨‹å¼é‚è¼¯ =================

if "messages" not in st.session_state:
    st.session_state.messages = []

if "vector_db" not in st.session_state:
    st.session_state.vector_db = None

with st.sidebar:
    st.header("ğŸ“ 1. è³‡æ–™ä¸Šå‚³")
    uploaded_files = st.file_uploader("è«‹ä¸Šå‚³ PDF æ–‡ä»¶", type="pdf", accept_multiple_files=True)
    
    if uploaded_files and st.session_state.vector_db is None:
        with st.spinner("â˜ï¸ æ­£åœ¨é›²ç«¯åˆ†æ PDF..."):
            try:
                import tempfile
                all_splits = []
                for uploaded_file in uploaded_files:
                    with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
                        tmp_file.write(uploaded_file.getvalue())
                        tmp_path = tmp_file.name

                    loader = PyPDFLoader(tmp_path)
                    docs = loader.load()
                    for doc in docs:
                        doc.metadata["source_filename"] = uploaded_file.name
                    
                    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
                    splits = text_splitter.split_documents(docs)
                    all_splits.extend(splits)
                    os.remove(tmp_path)

                if all_splits:
                    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/all-MiniLM-L6-v2")
                    vector_db = Chroma.from_documents(documents=all_splits, embedding=embeddings)
                    st.session_state.vector_db = vector_db
                    st.success(f"âœ… æˆåŠŸè™•ç† {len(uploaded_files)} ä»½æ–‡ä»¶ï¼")
                else:
                    st.warning("âš ï¸ æª”æ¡ˆå…§å®¹ç‚ºç©º")
            except Exception as e:
                st.error(f"âŒ è³‡æ–™è™•ç†éŒ¯èª¤: {e}")

    st.divider()
    st.header("âš™ï¸ 2. åƒæ•¸è¨­å®š")
    temperature = st.slider("å‰µæ„åº¦ (Temperature)", 0.0, 1.0, 0.1, 0.1)
    k_value = st.slider("æª¢ç´¢æ•¸ (Top-K)", 2, 10, 4)

    st.divider()
    if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±"):
        st.session_state.messages = []
        st.rerun()
    if st.button("ğŸ”„ é‡ç½®æ–‡ä»¶"):
        st.session_state.messages = []
        st.session_state.vector_db = None
        st.rerun()

for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

if prompt := st.chat_input("è«‹è¼¸å…¥å•é¡Œ..."):
    st.chat_message("user").markdown(prompt)
    st.session_state.messages.append({"role": "user", "content": prompt})

    if st.session_state.vector_db:
        with st.chat_message("assistant"):
            try:
                llm = ChatGroq(groq_api_key=GROQ_API_KEY, model_name="llama-3.3-70b-versatile", temperature=temperature)
                
                qa_prompt = ChatPromptTemplate.from_template("""
                ä½ æ˜¯ä¸€å€‹å°ˆæ¥­åŠ©ç†ã€‚è«‹æ ¹æ“šã€ä¸Šä¸‹æ–‡ã€‘å›ç­”å•é¡Œã€‚è‹¥ä¸çŸ¥é“è«‹èªªä¸çŸ¥é“ã€‚
                ã€ä¸Šä¸‹æ–‡ã€‘:{context}
                ã€å•é¡Œã€‘:{input}
                è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼š
                """)

                retriever = st.session_state.vector_db.as_retriever(search_kwargs={"k": k_value})
                document_chain = create_stuff_documents_chain(llm, qa_prompt)
                retrieval_chain = create_retrieval_chain(retriever, document_chain)
                
                response = retrieval_chain.invoke({"input": prompt})
                answer = response['answer']
                st.markdown(answer)
                st.session_state.messages.append({"role": "assistant", "content": answer})
                
                with st.expander("ğŸ” åƒè€ƒä¾†æº"):
                    for i, doc in enumerate(response['context']):
                        st.markdown(f"**ä¾†æº {i+1}: {doc.metadata.get('source_filename')} (p.{doc.metadata.get('page',0)+1})**")
                        st.text(doc.page_content[:200] + "...")
                        st.divider()
            except Exception as e:
                st.error(f"âŒ ç”Ÿæˆå›ç­”éŒ¯èª¤: {e}")
                if "API_KEY" in str(e) or "401" in str(e):
                    st.warning("è«‹æª¢æŸ¥ Secrets ä¸­çš„ API Key æ˜¯å¦æ­£ç¢ºã€‚")
    else:
        st.warning("âš ï¸ è«‹å…ˆä¸Šå‚³ PDF")